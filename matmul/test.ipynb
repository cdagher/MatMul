{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Array, Int, PyTree\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random as jrandom\n",
    "\n",
    "import equinox as eqx\n",
    "from equinox import Module\n",
    "\n",
    "import optax as opt\n",
    "\n",
    "from src.nn import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import streamlit as st\n",
    "from src.nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 2000\n",
    "PRINT_EVERY = 100\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def load_data(b: int = 100, key: jrandom.PRNGKey = jrandom.PRNGKey(0)):\n",
    "    key, subkey = jrandom.split(key)\n",
    "    X = jrandom.normal(subkey, (b, 2, 10, 10))\n",
    "    Y = X[:, 0] @ X[:, 1]\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and Training\n",
    "def loss(model: NN, x: Float[Array, \"b 2 10 10\"], y: Float[Array, \"b 10 10\"]) -> Float[Array, \"\"]:\n",
    "    y_pred = jax.vmap(model)(x[:, 0], x[:, 1])\n",
    "    return jnp.mean((y - y_pred) ** 2)\n",
    "    # return cross_entropy(y, y_pred)\n",
    "\n",
    "def cross_entropy(y: Float[Array, \"b 10 10\"], y_pred: Float[Array, \"b 10 10\"]) -> Float[Array, \"\"]:\n",
    "    return -jnp.mean(y * jnp.log(y_pred))\n",
    "\n",
    "def accuracy(y: Float[Array, \"b 10 10\"], y_pred: Float[Array, \"b 10 10\"]) -> Float[Array, \"\"]:\n",
    "    return jnp.mean(y == y_pred)\n",
    "\n",
    "def evaluate(model: NN, b: int = 100, key: jrandom.PRNGKey = jrandom.PRNGKey(0)) -> Float[Array, \"\"]:\n",
    "    X, Y = load_data(b, key)\n",
    "    return loss(model, X, Y)\n",
    "\n",
    "def train(\n",
    "        model: NN,\n",
    "        optim: opt.GradientTransformation,\n",
    "        steps: int,\n",
    "        print_every: int\n",
    ") -> NN:\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(\n",
    "        model: NN,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"b 2 10 10\"],\n",
    "        y: Float[Array, \"b 10 10\"]\n",
    "    ) -> NN:\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_step = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_step, loss_value\n",
    "    \n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            X, Y = load_data(BATCH_SIZE)\n",
    "            yield X, Y\n",
    "\n",
    "    for s, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        model, opt_state, loss_value = step(model, opt_state, x, y)\n",
    "\n",
    "        if s % print_every == 0 or s == steps - 1:\n",
    "            print(f\"Step {s}, Loss: {loss_value}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN\n",
      "Network parameters:  79588 \n",
      "\n",
      "Step 0, Loss: 10.12450885772705\n",
      "Step 100, Loss: 8.0065279006958\n",
      "Step 200, Loss: 3.3893675804138184\n",
      "Step 300, Loss: 0.8956431150436401\n",
      "Step 400, Loss: 0.12749134004116058\n",
      "Step 500, Loss: 0.010441564954817295\n",
      "Step 600, Loss: 0.0008798211347311735\n",
      "Step 700, Loss: 8.334965241374448e-05\n",
      "Step 800, Loss: 8.176562005246524e-06\n",
      "Step 900, Loss: 7.860908226575702e-07\n",
      "Step 1000, Loss: 7.070565999356404e-08\n",
      "Step 1100, Loss: 5.716437989633505e-09\n",
      "Step 1200, Loss: 4.062150060768488e-10\n",
      "Step 1300, Loss: 3.002399717733084e-11\n",
      "Step 1400, Loss: 4.542908150356739e-12\n",
      "Step 1500, Loss: 1.6038366381504465e-12\n",
      "Step 1600, Loss: 8.326447712737883e-13\n",
      "Step 1700, Loss: 5.250597049158423e-13\n",
      "Step 1800, Loss: 3.983249006241779e-13\n",
      "Step 1900, Loss: 3.163669142196984e-13\n",
      "Step 1999, Loss: 2.759897616434054e-13\n",
      "\n",
      "Network parameters:  79588 \n",
      "\n",
      "CNN(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[4,2,4,4],\n",
      "      bias=f32[4,1,1],\n",
      "      in_channels=2,\n",
      "      out_channels=4,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[6,4,2,2],\n",
      "      bias=f32[6,1,1],\n",
      "      in_channels=4,\n",
      "      out_channels=6,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True,\n",
      "      padding_mode='ZEROS'\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    <wrapped function ravel>,\n",
      "    split_layer(\n",
      "      layers=[\n",
      "        Linear(\n",
      "          weight=f32[25,216],\n",
      "          bias=None,\n",
      "          in_features=216,\n",
      "          out_features=25,\n",
      "          use_bias=False\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[250,25],\n",
      "          bias=f32[250],\n",
      "          in_features=25,\n",
      "          out_features=250,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ]\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Linear(\n",
      "      weight=f32[100,250],\n",
      "      bias=f32[100],\n",
      "      in_features=250,\n",
      "      out_features=100,\n",
      "      use_bias=True\n",
      "    )\n",
      "  ]\n",
      ") params: 25338 \n",
      "\n",
      "Step 0, Loss: 6.911667346954346\n",
      "Step 100, Loss: 2.7495174407958984\n",
      "Step 200, Loss: 1.880798578262329\n",
      "Step 300, Loss: 1.1155813932418823\n",
      "Step 400, Loss: 0.5318548679351807\n",
      "Step 500, Loss: 0.20598644018173218\n",
      "Step 600, Loss: 0.06816248595714569\n",
      "Step 700, Loss: 0.02125401422381401\n",
      "Step 800, Loss: 0.00667556282132864\n",
      "Step 900, Loss: 0.002143960213288665\n",
      "Step 1000, Loss: 0.0007047158433124423\n",
      "Step 1100, Loss: 0.00023292294645216316\n",
      "Step 1200, Loss: 7.640202238690108e-05\n",
      "Step 1300, Loss: 2.4629433028167114e-05\n",
      "Step 1400, Loss: 7.74101590650389e-06\n",
      "Step 1500, Loss: 2.358978917982313e-06\n",
      "Step 1600, Loss: 6.926040896360064e-07\n",
      "Step 1700, Loss: 1.947646666167202e-07\n",
      "Step 1800, Loss: 5.218761600644939e-08\n",
      "Step 1900, Loss: 1.3265816889429516e-08\n",
      "Step 2000, Loss: 3.196334308341875e-09\n",
      "Step 2100, Loss: 7.407298663331119e-10\n",
      "Step 2200, Loss: 1.7383537664894533e-10\n",
      "Step 2300, Loss: 4.648405271812095e-11\n",
      "Step 2400, Loss: 1.633810794987589e-11\n",
      "Step 2500, Loss: 7.581932477684905e-12\n",
      "Step 2600, Loss: 4.1852142411424875e-12\n",
      "Step 2700, Loss: 2.743003523972276e-12\n",
      "Step 2800, Loss: 1.928577306534174e-12\n",
      "Step 2900, Loss: 1.4198127265899196e-12\n",
      "Step 3000, Loss: 1.1153054391491168e-12\n",
      "Step 3100, Loss: 1.0123069917045147e-12\n",
      "Step 3200, Loss: 9.501902303171716e-13\n",
      "Step 3300, Loss: 7.997923070351964e-13\n",
      "Step 3400, Loss: 2.415250946796732e-06\n",
      "Step 3500, Loss: 3.4443203844602976e-10\n",
      "Step 3600, Loss: 7.520464286037143e-12\n",
      "Step 3700, Loss: 1.6490575181424272e-12\n",
      "Step 3800, Loss: 8.778100316943205e-13\n",
      "Step 3900, Loss: 7.337985470992625e-13\n",
      "Step 3999, Loss: 7.3215034295665e-13\n"
     ]
    }
   ],
   "source": [
    "print(\"Training CNN\")\n",
    "\n",
    "net = CNN()\n",
    "\n",
    "print(\"Network parameters: \", net.param_count(), \"\\n\")\n",
    "\n",
    "net = train(net, opt.adam(LEARNING_RATE), EPOCHS, PRINT_EVERY)\n",
    "\n",
    "print(\"\\nNetwork parameters: \", net.param_count(), \"\\n\")\n",
    "net.split_layer(5, 25)\n",
    "\n",
    "print(net, \"\\n\")\n",
    "\n",
    "net = train(net, opt.adam(LEARNING_RATE), 2*EPOCHS, PRINT_EVERY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatMul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
